## TASK 1: LLM-Powered AI Prototype

# ğŸ“„ PDF AI Assistant

A full-stack application that allows users to upload PDF documents and interact with them using natural language. Built with **FastAPI**, **React**, **LangChain**, and **Hugging Face**.

## ğŸš€ Features

- **Deployed & Live**: Fully hosted on Hugging Face Spaces (Backend) and GitHub Pages (Frontend).

- **PDF Upload & Processing**: Automatically extracts text, chunks it, and generates vector embeddings.
- **RAG Pipeline**: Uses Retrieval-Augmented Generation to answer questions based *only* on the PDF content.
- **LLM Integration**: Powered by **Meta LLaMA 3.1 8B Instruct** (via Hugging Face API) for high-quality responses.
- **Vector Search**: Efficient similarity search using **FAISS** (with in-memory fallback).
- **Interactive Chat UI**:
  - Real-time chat interface.
  - **Markdown Support**: Renders lists, code blocks, and bold text properly.
  - **Auto-scroll**: Keeps the latest message in view.
  - **Persistence**: Chat history is saved locally so you don't lose progress on refresh.
- **Responsive Design**: Beautiful, modern UI with gradient styling.

### ğŸš€ Live Demo ğŸ‘‰ [Try the app here](https://your-render-app.onrender.com

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI
- **Hosting**: Hugging Face Spaces (Docker)
- **LLM Orchestration**: LangChain
- **Model Provider**: Hugging Face Inference API
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` (Local)
- **Vector Store**: FAISS (Facebook AI Similarity Search)

### Frontend
- **Library**: React.js
- **Hosting**: GitHub Pages
- **Styling**: CSS3 (Custom gradients & animations)
- **HTTP Client**: Axios
- **Rendering**: `react-markdown`

## ğŸ“‹ Prerequisites

- Python 3.8+
- Node.js & npm
- A [Hugging Face Account](https://huggingface.co/) & API Token.
- Access to `meta-llama/Meta-Llama-3.1-8B-Instruct` (Accept license on HF model page).

## âš™ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone https://github.com/divyansshu/Automating-PDF-Interaction.git
cd "Automating PDF Interaction"
```

### 2. Backend Setup
Navigate to the backend folder and install dependencies:
```bash
cd backend
# Create virtual environment (optional but recommended)
python -m venv venv
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install requirements
pip install -r requirements.txt
```

**Configure Environment Variables:**
Create a `.env` file in `backend/app/.env`:
```env
HUGGINGFACEHUB_API_TOKEN=your_hf_token_here
```

**Run the Server:**
```bash
cd app
uvicorn main:app --reload
```
The backend will start at `http://localhost:8000`.

### 3. Frontend Setup
Open a new terminal and navigate to the frontend folder:
```bash
cd frontend
npm install
npm start
```
The app will open at `http://localhost:3000`.

## ğŸ’¡ Usage

1.  Open the web app.
2.  Click **"Upload PDF"** and select a document.
3.  Wait for the processing (extraction & embedding generation).
4.  Once done, you will be redirected to the chat screen.
5.  Ask any question about your PDF!
6.  Use the **"Upload New PDF"** button to reset and start over.

## ğŸ›¡ï¸ License

This project is open-source and available under the MIT License.

## Workflow
**Upload PDF â†’ Extract text (pdfplumber).
Chunk text â†’ Overlapping segments.
Embed chunks â†’ OpenAIEmbeddings or SentenceTransformers.
Store in Vector DB â†’ FAISS.
User query â†’ Embed query, retrieve top-k chunks.
Prompt â†’ Insert retrieved context into LLM prompt.
Response â†’ Display in Streamlit chat UI.

## Design Choices
LLM: GPT for reliability, LLaMA2 for cost/privacy.
Vector DB: FAISS (fast, local) â†’ Pinecone for enterprise scaling.
Chunking: Overlap prevents broken sentences/context loss.
Prompt: Source-grounded to reduce hallucination.
UI: Streamlit for demo speed.

### Add Requirement files.
**streamlit**
**langchain**
**openai**
**faiss-cpu**
**PyPDF2**
**pdfplumber**

### Project Structure 
â”œâ”€â”€ app.py              # Streamlit/FastAPI entry point
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ /docs               # Architecture diagrams, notes
â””â”€â”€ /src                # Core code (chunking, embeddings, RAG pipeline)

## Add Architecture Diagram
[PDF Upload] â†’ [Chunking] â†’ [Embeddings â†’ Vector DB] â†’ [Retriever] â†’ [LLM] â†’ [UI]

### Workflow 1. **Upload PDF** â†’ Extract text (`pdfplumber` / `PyPDF2`). 2. **Chunking** â†’ Sliding window (~1000 characters, 200 overlap). 3. **Embeddings** â†’ `OpenAIEmbeddings` or `SentenceTransformers`. 4. **Vector DB** â†’ FAISS for local storage. 5. **Query** â†’ Embed user query, retrieve top-k chunks. 6. **Prompt Engineering** â†’ Insert retrieved context into LLM prompt. 7. **Response** â†’ Display in Streamlit chat UI. ### Design Choices - **LLM**: GPT for reliability; LLaMA2/Mistral for cost/privacy. - **Vector DB**: FAISS (fast, local) â†’ Pinecone for production scaling. - **Chunking**: Overlap prevents broken context. - **Prompt Engineering**: Source-grounded, avoids hallucination. - **UI**: Streamlit for rapid prototyping.

## ğŸ›¡ï¸ Task 2: Hallucination & Quality Control

### Causes of Hallucination
- LLMs may generate confident but incorrect answers when:
  - Context is missing or incomplete.
  - Retrieval returns irrelevant chunks.
  - Prompts are ambiguous or unconstrained.

### Guardrails Implemented
1. **Confidence Thresholds**  
   - Responses are only generated if similarity score > threshold.  
   - Otherwise, the system replies: *â€œNot found in document.â€*

2. **Source-Grounded Answers**  
   - All answers are explicitly tied to retrieved chunks.  
   - Prompt constraint: *â€œUse only the provided context. Do not invent information.â€*

3. **Prompt Constraints**  
   - Instructions force the model to avoid speculation.  
   - Example: *â€œIf unsure, say â€˜Not found in document.â€™â€*

### Example of Improved Responses
- **Before (Hallucination)**:  
  *â€œThe company was founded in 1990.â€*  
- **After (Guardrail Applied)**:  
  *â€œThe founding year is not mentioned in the document. Closest reference is early operations.â€*

---

## âš¡ Task 3: Rapid Iteration Challenge

### Advanced Capability: Multi-Document Reasoning
**Why chosen**: Real-world use cases often involve multiple PDFs (contracts, resumes, reports).  
**Implementation**:  
- Ingest multiple PDFs â†’ Merge embeddings into one vector DB.  
- Retrieval â†’ Query across all documents simultaneously.  
- Prompt â†’ Include source identifiers (e.g., Doc A, Doc B).  

**Trade-offs**:
- âœ… Richer, enterprise-ready answers.  
- âŒ Higher compute cost and retrieval complexity.  
- ğŸ”’ Limitation: Requires metadata filtering for relevance.

---

## ğŸ¢ Task 4: AI System Architecture

### Enterprise Assistant Design

**Components:**
- **Data Ingestion**: ETL pipeline (PDFs, docs, emails â†’ text).  
- **Vector DB Choice**: Pinecone/Weaviate for scalability and metadata filtering.  
- **LLM Orchestration**: LangChain/LlamaIndex for RAG pipeline management.  
- **Cost Control**:  
  - Cache embeddings.  
  - Use smaller LLM for retrieval, larger LLM for final answer.  
- **Monitoring & Evaluation**:  
  - Track query success rate.  
  - Log hallucinations.  
  - Human feedback loop for continuous improvement.

### Architecture Diagram
[Data Sources: PDFs, Docs, Emails]
        â†“
 [ETL + Chunking]
        â†“
 [Embeddings â†’ Vector DB (Pinecone)]
        â†“
 [Retriever â†’ Top-k Chunks]
        â†“
 [LLM Orchestration (LangChain)]
        â†“
 [Response Generation + Guardrails]
        â†“
 [UI Layer (Streamlit / FastAPI)]
        â†“
 [Monitoring + Feedback Loop]

### Project Structure
â”œâ”€â”€ app.py              # Streamlit app entry point
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ /docs               # Architecture diagrams, notes
â””â”€â”€ /src                # Core code (chunking, embeddings, RAG pipeline)

---

## âœ¨ Features
- LLM-powered PDF Q&A  
- RAG with FAISS  
- Chunking strategy  
- Guardrails against hallucination  
- Multi-document reasoning  
- Enterprise-ready architecture  

---

## ğŸ“ Author
**Anmol Tomar** â€“ MCA AIML, 590019134  
Focus: LLMs, RAG, NLP, and enterprise AI systems.
