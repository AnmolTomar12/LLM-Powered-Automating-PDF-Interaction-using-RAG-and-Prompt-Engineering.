## TASK 1: LLM-Powered AI Prototype

# ğŸ“„ PDF AI Assistant

A full-stack application that allows users to upload PDF documents and interact with them using natural language. Built with **FastAPI**, **React**, **LangChain**, and **Hugging Face**.

## ğŸš€ Features

- **Deployed & Live**: Fully hosted on Hugging Face Spaces (Backend) and GitHub Pages (Frontend).

- **PDF Upload & Processing**: Automatically extracts text, chunks it, and generates vector embeddings.
- **RAG Pipeline**: Uses Retrieval-Augmented Generation to answer questions based *only* on the PDF content.
- **LLM Integration**: Powered by **Meta LLaMA 3.1 8B Instruct** (via Hugging Face API) for high-quality responses.
- **Vector Search**: Efficient similarity search using **FAISS** (with in-memory fallback).
- **Interactive Chat UI**:
  - Real-time chat interface.
  - **Markdown Support**: Renders lists, code blocks, and bold text properly.
  - **Auto-scroll**: Keeps the latest message in view.
  - **Persistence**: Chat history is saved locally so you don't lose progress on refresh.
- **Responsive Design**: Beautiful, modern UI with gradient styling.

### ğŸš€ Live Demo ğŸ‘‰ https://llm-powered-automating-pdf-interaction.onrender.com

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI
- **Hosting**: Hugging Face Spaces (Docker)
- **LLM Orchestration**: LangChain
- **Model Provider**: Hugging Face Inference API
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` (Local)
- **Vector Store**: FAISS (Facebook AI Similarity Search)

### Frontend
- **Library**: React.js
- **Hosting**: GitHub Pages
- **Styling**: CSS3 (Custom gradients & animations)
- **HTTP Client**: Axios
- **Rendering**: `react-markdown`

## ğŸ“‹ Prerequisites

- Python 3.8+
- Node.js & npm
- A [Hugging Face Account](https://huggingface.co/) & API Token.
- Access to `meta-llama/Meta-Llama-3.1-8B-Instruct` (Accept license on HF model page).

## âš™ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone https://github.com/divyansshu/Automating-PDF-Interaction.git
cd "Automating PDF Interaction"
```

### 2. Backend Setup
Navigate to the backend folder and install dependencies:
```bash
cd backend
# Create virtual environment (optional but recommended)
python -m venv venv
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install requirements
pip install -r requirements.txt
```

**Configure Environment Variables:**
Create a `.env` file in `backend/app/.env`:
```env
HUGGINGFACEHUB_API_TOKEN=your_hf_token_here
```

**Run the Server:**
```bash
cd app
uvicorn main:app --reload
```
The backend will start at `http://localhost:8000`.

### 3. Frontend Setup
Open a new terminal and navigate to the frontend folder:
```bash
cd frontend
npm install
npm start
```
The app will open at `http://localhost:3000`.

## ğŸ’¡ Usage

1.  Open the web app.
2.  Click **"Upload PDF"** and select a document.
3.  Wait for the processing (extraction & embedding generation).
4.  Once done, you will be redirected to the chat screen.
5.  Ask any question about your PDF!
6.  Use the **"Upload New PDF"** button to reset and start over.

## ğŸ›¡ï¸ License

This project is open-source and available under the MIT License.

## ğŸ”„ Workflow

Think of the system as a **conversation pipeline** where your PDF transforms into an interactive knowledge base. Hereâ€™s how the magic happens:

1. ğŸ“¤ **Upload PDF** â†’ User uploads a document.  
2. ğŸ“‘ **Text Extraction** â†’ Content is extracted using `pdfplumber` / `PyPDF2`.  
3. âœ‚ï¸ **Chunking** â†’ Text is split into overlapping segments (~1000 characters, 200 overlap) to preserve context.  
4. ğŸ§  **Embeddings** â†’ Each chunk is converted into dense vectors using `OpenAIEmbeddings` or `SentenceTransformers`.  
5. ğŸ—‚ï¸ **Vector Database** â†’ Embeddings are stored in **FAISS** (local) or **Pinecone** (scalable cloud).  
6. â“ **User Query** â†’ Query is embedded and matched against the most relevant chunks.  
7. ğŸ“ **Prompt Engineering** â†’ Retrieved context is inserted into a carefully designed prompt.  
8. ğŸ¤– **LLM Response** â†’ GPT (or LLaMA2/Mistral) generates an answer grounded in the PDF.  
9. ğŸ’¬ **UI Display** â†’ Streamlit chat interface shows the response with conversational flow.  

---

## ğŸ¨ Design Choices

- **LLM**:  
  - ğŸŸ¢ *OpenAI GPT* â†’ Reliable, strong reasoning.  
  - ğŸŸ¡ *LLaMA2/Mistral* â†’ Cost-effective, privacy-friendly alternatives.  

- **Vector DB**:  
  - âš¡ *FAISS* â†’ Fast, local prototyping.  
  - â˜ï¸ *Pinecone* â†’ Enterprise scaling with metadata filtering.  

- **Chunking Strategy**:  
  - Sliding window with overlap ensures no sentence or context is cut off mid-thought.  

- **Prompt Engineering**:  
  - Source-grounded prompts reduce hallucination.  
  - Explicit instructions: *â€œIf unsure, say â€˜Not found in document.â€™â€*  

- **UI**:  
  - ğŸš€ *Streamlit* â†’ Rapid prototyping with minimal setup.  
  - ğŸ”— *FastAPI* â†’ Optional REST endpoints for production integration.  

---

## ğŸ“¦ Requirements

##Add the following dependencies to `requirements.txt`:

```txt
streamlit
langchain
openai
faiss-cpu
PyPDF2
pdfplumber

### Project Structure

â”œâ”€â”€ app.py              # Streamlit/FastAPI entry point
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ /docs               # Architecture diagrams, notes
â””â”€â”€ /src                # Core code (chunking, embeddings, RAG pipeline)

## Add Architecture Diagram
[PDF Upload] â†’ [Chunking] â†’ [Embeddings â†’ Vector DB] â†’ [Retriever] â†’ [LLM] â†’ [UI]

## ğŸ›¡ï¸ Task 2: Hallucination & Quality Control

### Causes of Hallucination
- LLMs may generate confident but incorrect answers when:
  - Context is missing or incomplete.
  - Retrieval returns irrelevant chunks.
  - Prompts are ambiguous or unconstrained.

### Guardrails Implemented
1. **Confidence Thresholds**  
   - Responses are only generated if similarity score > threshold.  
   - Otherwise, the system replies: *â€œNot found in document.â€*

2. **Source-Grounded Answers**  
   - All answers are explicitly tied to retrieved chunks.  
   - Prompt constraint: *â€œUse only the provided context. Do not invent information.â€*

3. **Prompt Constraints**  
   - Instructions force the model to avoid speculation.  
   - Example: *â€œIf unsure, say â€˜Not found in document.â€™â€*

### Example of Improved Responses
- **Before (Hallucination)**:  
  *â€œThe company was founded in 1990.â€*  
- **After (Guardrail Applied)**:  
  *â€œThe founding year is not mentioned in the document. Closest reference is early operations.â€*

---

## âš¡ Task 3: Rapid Iteration Challenge

### Advanced Capability: Multi-Document Reasoning
**Why chosen**: Real-world use cases often involve multiple PDFs (contracts, resumes, reports).  
**Implementation**:  
- Ingest multiple PDFs â†’ Merge embeddings into one vector DB.  
- Retrieval â†’ Query across all documents simultaneously.  
- Prompt â†’ Include source identifiers (e.g., Doc A, Doc B).  

**Trade-offs**:
- âœ… Richer, enterprise-ready answers.  
- âŒ Higher compute cost and retrieval complexity.  
- ğŸ”’ Limitation: Requires metadata filtering for relevance.

---

## ğŸ¢ Task 4: AI System Architecture

### Enterprise Assistant Design

**Components:**
- **Data Ingestion**: ETL pipeline (PDFs, docs, emails â†’ text).  
- **Vector DB Choice**: Pinecone/Weaviate for scalability and metadata filtering.  
- **LLM Orchestration**: LangChain/LlamaIndex for RAG pipeline management.  
- **Cost Control**:  
  - Cache embeddings.  
  - Use smaller LLM for retrieval, larger LLM for final answer.  
- **Monitoring & Evaluation**:  
  - Track query success rate.  
  - Log hallucinations.  
  - Human feedback loop for continuous improvement.

### Architecture Diagram
[Data Sources: PDFs, Docs, Emails]
        â†“
 [ETL + Chunking]
        â†“
 [Embeddings â†’ Vector DB (Pinecone)]
        â†“
 [Retriever â†’ Top-k Chunks]
        â†“
 [LLM Orchestration (LangChain)]
        â†“
 [Response Generation + Guardrails]
        â†“
 [UI Layer (Streamlit / FastAPI)]
        â†“
 [Monitoring + Feedback Loop]

### Project Structure
â”œâ”€â”€ app.py              # Streamlit app entry point
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ /docs               # Architecture diagrams, notes
â””â”€â”€ /src                # Core code (chunking, embeddings, RAG pipeline)

---

## âœ¨ Features
- LLM-powered PDF Q&A  
- RAG with FAISS  
- Chunking strategy  
- Guardrails against hallucination  
- Multi-document reasoning  
- Enterprise-ready architecture  

---

## ğŸ“ Author
**Anmol Tomar** â€“ MCA AIML, 590019134  
Focus: LLMs, RAG, NLP, and enterprise AI systems.
